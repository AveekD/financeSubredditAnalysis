{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "posts_list = pd.read_pickle('posts_investing_year.pkl')\n",
    "\n",
    "df = pd.DataFrame(posts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
       "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
       "       'awarders', 'can_mod_post', 'contest_mode', 'created_utc', 'domain',\n",
       "       'full_link', 'gildings', 'id', 'is_crosspostable', 'is_meta',\n",
       "       'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable',\n",
       "       'is_self', 'is_video', 'link_flair_background_color',\n",
       "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
       "       'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts',\n",
       "       'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
       "       'steward_reports', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_subscribers', 'subreddit_type', 'thumbnail', 'title',\n",
       "       'total_awards_received', 'url', 'whitelist_status', 'wls', 'removed_by',\n",
       "       'suggested_sort', 'post_hint', 'preview',\n",
       "       'author_flair_background_color', 'author_flair_text_color', 'edited',\n",
       "       'banned_by', 'author_cakeday', 'updated_utc', 'og_description',\n",
       "       'og_title', 'gilded', 'distinguished', 'media_metadata',\n",
       "       'thumbnail_height', 'thumbnail_width', 'link_flair_css_class',\n",
       "       'link_flair_template_id', 'link_flair_text', 'author_created_utc',\n",
       "       'author_flair_template_id', 'category', 'content_categories',\n",
       "       'media_embed', 'removal_reason', 'secure_media_embed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new type for df\n",
    "df['text'] = df['title'] + ' ' + df['selftext']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Suggested readings for new investor Hi r/inves...\n",
       "1        Charles Schwab vs M1 vs Robinhood I’ve had a R...\n",
       "2        Are you guys maxing out your IRAs Jan 1? With ...\n",
       "3        Short Stock Question Let’s say you have 50k. I...\n",
       "4        If you had a 100k cash, what would you do with...\n",
       "                               ...                        \n",
       "27995    Is this a bad idea: buying/holding inverse S&a...\n",
       "27996    Rent/bills due by the 26 and still need 400EUR...\n",
       "27997    Invested in a company, can't contact, no news,...\n",
       "27998    Rent/bills due by the 26 and still need 400EUR...\n",
       "27999    Can certain or a few bunch of stocks make mone...\n",
       "Name: text, Length: 28000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "new_stop_words = set(['removed','x200b', 'amp', 'hi', 'like', 'get'])\n",
    "stop_words = stop_words.union(new_stop_words)\n",
    "corpus = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df = df[df['author'] != 'AutoModerator']\n",
    "\n",
    "for x in df['text']:\n",
    "    text = str(x).lower()\n",
    "    text = tokenizer.tokenize(re.sub(r'https?://\\S+', '', text))\n",
    "    ps=PorterStemmer()\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suggested',\n",
       " 'reading',\n",
       " 'new',\n",
       " 'investor',\n",
       " 'investing',\n",
       " 'looking',\n",
       " 'learn',\n",
       " 'particularly',\n",
       " 'respect',\n",
       " 'stock']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "#Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Bi-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "print(top2_df)\n",
    "#Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)\n",
    "#Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "doc=corpus[532]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import random \n",
    "import time\n",
    "random.seed(time.time())\n",
    "\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    "count = 0\n",
    "results = []\n",
    "for x in corpus:\n",
    "    # fetch document for which keywords needs to be extracted\n",
    "    doc=x\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "    #Function for sorting tf_idf in descending order\n",
    "    from scipy.sparse import coo_matrix\n",
    "    def sort_coo(coo_matrix):\n",
    "        tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "        return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "    def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "        \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "\n",
    "        #use only topn items from vector\n",
    "        sorted_items = sorted_items[:topn]\n",
    "\n",
    "        score_vals = []\n",
    "        feature_vals = []\n",
    "\n",
    "        # word index and corresponding tf-idf score\n",
    "        for idx, score in sorted_items:\n",
    "\n",
    "            #keep track of feature name and its corresponding score\n",
    "            score_vals.append(round(score, 3))\n",
    "            feature_vals.append(feature_names[idx])\n",
    "\n",
    "        #create a tuples of feature,score\n",
    "        #results = zip(feature_vals,score_vals)\n",
    "        results= {}\n",
    "        for idx in range(len(feature_vals)):\n",
    "            results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "        return results\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "    \n",
    "    #list of desired keywords\n",
    "    market_key_words = set(['market','stock', 'long term', 'calls', 'puts', 'invest', 'roth ira', 'buy', 'sell', 'growth', 'etf', 'dow', 'short term', 'stock market', 'short', 'bear', 'bull', 'correction', 'recession', 'total stock market', 'trade war', '10 year', 'stock trading','market index fund' , 'interest rate', 'index fund'])\n",
    "    # now print the results\n",
    "    found = 0\n",
    "    for k in keywords:\n",
    "        if(found == 0):\n",
    "            for m in market_key_words:\n",
    "                if(m == k):\n",
    "                    count+=1\n",
    "                    found = 1\n",
    "                    results.append(x)\n",
    "                    break\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# New words and values\n",
    "new_words = {\n",
    "    'buy': 5, \n",
    "    'crushes': 10,\n",
    "    'long term': 10,\n",
    "    'beats': 5,\n",
    "    'buy calls': 5,\n",
    "    'short term': 1,\n",
    "    'misses': -5,\n",
    "    'sell': -5,\n",
    "    'buy puts': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -10,\n",
    "}\n",
    "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# Update the lexicon\n",
    "# ... YOUR CODE FOR TASK 4 ...\n",
    "vader.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "results_sentiment=[]\n",
    "for x in results:\n",
    "    print(x)\n",
    "    print(\"Sentiment Score: \", vader.polarity_scores(x)['compound'])\n",
    "    print(\"\")\n",
    "    results_sentiment.append(vader.polarity_scores(x)['compound'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis([0, len(results), -1, 1])\n",
    "plt.plot(results_sentiment)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
